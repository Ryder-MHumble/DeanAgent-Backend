from __future__ import annotations

import difflib
import hashlib
import logging
import re
from datetime import datetime, timezone

from bs4 import BeautifulSoup
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.crawlers.base import BaseCrawler, CrawledItem
from app.crawlers.utils.http_client import fetch_page
from app.models.snapshot import Snapshot

logger = logging.getLogger(__name__)


class SnapshotDiffCrawler(BaseCrawler):
    """
    Crawler for pages without news lists (leadership rosters, faculty directories).
    Compares page content hash with last stored snapshot to detect changes.

    Config fields:
      - url: target page URL
      - selectors:
          content_area: CSS selector for meaningful content area
      - ignore_patterns: regex patterns to strip before hashing (timestamps, counters)
      - headers, encoding, request_delay: same as StaticHTMLCrawler
    """

    async def fetch_and_parse(self) -> list[CrawledItem]:
        # This crawler needs DB access for snapshots, handled specially in run()
        raise NotImplementedError("Use run() directly; fetch_and_parse requires db_session")

    async def run(self, db_session: AsyncSession):
        """Override run() to handle snapshot comparison with DB."""
        from app.crawlers.base import CrawlResult, CrawlStatus

        result = CrawlResult(source_id=self.source_id)
        result.started_at = datetime.now(timezone.utc)

        try:
            url = self.config["url"]
            selectors = self.config.get("selectors", {})
            ignore_patterns = self.config.get("ignore_patterns", [])

            html = await fetch_page(
                url,
                headers=self.config.get("headers"),
                encoding=self.config.get("encoding"),
                request_delay=self.config.get("request_delay"),
            )

            # Extract content area
            soup = BeautifulSoup(html, "lxml")
            content_area_sel = selectors.get("content_area")
            if content_area_sel:
                el = soup.select_one(content_area_sel)
                text = el.get_text(separator="\n").strip() if el else ""
            else:
                text = soup.get_text(separator="\n").strip()

            # Apply ignore patterns
            for pattern in ignore_patterns:
                text = re.sub(pattern, "", text)

            # Compute hash
            content_hash = hashlib.sha256(text.encode("utf-8")).hexdigest()

            # Get last snapshot
            stmt = (
                select(Snapshot)
                .where(Snapshot.source_id == self.source_id)
                .order_by(Snapshot.captured_at.desc())
                .limit(1)
            )
            last_snapshot = (await db_session.execute(stmt)).scalar_one_or_none()

            if last_snapshot and last_snapshot.content_hash == content_hash:
                # No change
                result.status = CrawlStatus.NO_NEW_CONTENT
                result.items_total = 0
                result.items_new = 0
            else:
                # Content changed (or first snapshot)
                diff_text = None
                if last_snapshot and last_snapshot.content_text:
                    diff_lines = difflib.unified_diff(
                        last_snapshot.content_text.splitlines(),
                        text.splitlines(),
                        lineterm="",
                    )
                    diff_text = "\n".join(diff_lines)

                # Store new snapshot
                new_snapshot = Snapshot(
                    source_id=self.source_id,
                    content_hash=content_hash,
                    content_text=text,
                    diff_text=diff_text,
                )
                db_session.add(new_snapshot)

                # Create a CrawledItem for the change
                # Append content_hash fragment to make each change a unique URL for dedup
                title = f"[变更检测] {self.config.get('name', self.source_id)}"
                item_url = f"{url}#snapshot-{content_hash[:12]}"
                item = CrawledItem(
                    title=title,
                    url=item_url,
                    content=diff_text or f"初次快照: {text[:500]}",
                    content_hash=content_hash,
                    source_id=self.source_id,
                    dimension=self.config.get("dimension"),
                    tags=self.config.get("tags", []) + ["snapshot_diff"],
                    extra={"is_first_snapshot": last_snapshot is None},
                )
                result.items = [item]
                result.items_total = 1
                result.items_new = 1
                result.status = CrawlStatus.SUCCESS

        except Exception as e:
            logger.exception("Snapshot crawl failed for %s", self.source_id)
            result.status = CrawlStatus.FAILED
            result.error_message = str(e)
        finally:
            result.finished_at = datetime.now(timezone.utc)
            result.duration_seconds = (result.finished_at - result.started_at).total_seconds()

        return result
